{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udacity Write A Data Science Blog Post Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project is part of The [Udacity](https://eu.udacity.com/) Data Scientist Nanodegree Program which is composed by:\n",
    "- Term 1\n",
    "    - Supervised Learning\n",
    "    - Deep Learning\n",
    "    - Unsupervised Learning\n",
    "- Term 2\n",
    "    - Write A Data Science Blog Post\n",
    "    - \n",
    "    -\n",
    "    \n",
    "The CRISP-DM Process (Cross Industry Process for Data Mining):\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Prepare Data\n",
    "4. Data Modeling\n",
    "5. Evaluate the Results\n",
    "6. Deploy   \n",
    "\n",
    "The goal of this project is to put in practice the technical skills teached during the program but manly to focus on the ability to effectively communicate the results of the analysis.\n",
    "\n",
    "### Software and Libraries\n",
    "This project uses Python 3.7.2 the following libraries:\n",
    "- NumPy\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- Matplotlib\n",
    "- seaborn\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "Looking at the suggested datasets I was thinkg that I wanted to do something useful. With some friends we were pondering the idea to transfer in Milan to be closer to our working places. Milan is really a beatiful place but it is also very expensive. \n",
    "\n",
    "Questions:\n",
    "- Witch are the 5 best neighborhood?\n",
    "- Witch are the 5 worst neighborhood?\n",
    "- How much is different the overview of the neighborhood given from the hosts and the guests?\n",
    "\n",
    "So I have decided to use the Airbnb dataset for the city of Milan to do a sentiment analysis of the neighborhoods\n",
    "\n",
    "## Data Understanding\n",
    "\n",
    "As already said the dataset is provided by [Airbnb](http://insideairbnb.com/get-the-data.html) and is basicalyy composed by:\n",
    "- listings.csv:\tDetailed Listings data for Milan\n",
    "- calendar.csv:\tDetailed Calendar Data for listings in Milan\n",
    "- reviews.csv:\tDetailed Review Data for listings in Milan\n",
    "- summary_listings.csv:\tSummary information and metrics for listings in Milan (good for visualisations).\n",
    "- summary_reviews.csv: Summary Review data and Listing ID (to facilitate time based analytics and visualisations linked to a listing).\n",
    "- neighbourhoods.csv: Neighbourhood list for geo filter. Sourced from city or open source GIS files.\n",
    "- neighbourhoods.geojson: GeoJSON file of neighbourhoods of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "#import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "#from time import time\n",
    "#from collections import Counter\n",
    "\n",
    "#import nltk\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "data_folder = 'airbnb_data/'\n",
    "\n",
    "# Load the datasets\n",
    "df_listings_data = pd.read_csv(data_folder + 'listings.csv')\n",
    "df_calendar_data = pd.read_csv(data_folder + 'calendar.csv')\n",
    "df_reviews_data = pd.read_csv(data_folder + 'reviews.csv')\n",
    "#df_summary_listings_data = pd.read_csv(data_folder + 'summary_listings.csv')\n",
    "#df_summary_reviews_data = pd.read_csv(data_folder + 'summary_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numerical variables:\")\n",
    "\n",
    "for name, values in df_listings_data.iteritems():\n",
    "    if(values.dtype == np.float64 or values.dtype == np.int64):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical variables values:\")\n",
    "\n",
    "for name, values in df_listings_data.iteritems():\n",
    "    if(values.dtype != np.float64 and values.dtype != np.int64):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, values in df_listings_data.iteritems():\n",
    "    if(values.dtype != np.float64 and values.dtype != np.int64):\n",
    "        print('{name}: {value}\\n'.format(name=name, value=values.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numerical variables:\")\n",
    "\n",
    "for name, values in df_calendar_data.iteritems():\n",
    "    if(values.dtype == np.float64 or values.dtype == np.int64):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical variables values:\")\n",
    "\n",
    "for name, values in df_calendar_data.iteritems():\n",
    "    if(values.dtype != np.float64 and values.dtype != np.int64):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, values in df_calendar_data.iteritems():\n",
    "    if(values.dtype != np.float64 and values.dtype != np.int64):\n",
    "        print('{name}: {value}\\n'.format(name=name, value=values.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numerical variables:\")\n",
    "\n",
    "for name, values in df_reviews_data.iteritems():\n",
    "    if(values.dtype == np.float64 or values.dtype == np.int64):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical variables values:\")\n",
    "\n",
    "for name, values in df_reviews_data.iteritems():\n",
    "    if(values.dtype != np.float64 and values.dtype != np.int64):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, values in df_reviews_data.iteritems():\n",
    "    if(values.dtype != np.float64 and values.dtype != np.int64):\n",
    "        print('{name}: {value}\\n'.format(name=name, value=values.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at the columns of the datasets we can figure out which of them can be usefull to answer our questions, of course for our goal the main focus is on the neighbourhoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data_cleaned = df_listings_data[['id'\n",
    "                                             #, 'name'\n",
    "                                             #, 'summary'\n",
    "                                             #, 'space'\n",
    "                                             #, 'description'\n",
    "                                             , 'neighborhood_overview'\n",
    "                                             #, 'transit'\n",
    "                                             #, 'access'\n",
    "                                             #, 'interaction'\n",
    "                                             #, 'house_rules'\n",
    "                                             #, 'host_about'\n",
    "                                             #, 'host_neighbourhood'\n",
    "                                             #, 'neighbourhood'\n",
    "                                             , 'neighbourhood_cleansed']]\n",
    "\n",
    "df_listings_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data_cleaned.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_listings_data_cleaned['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'id' as key in the dataframe\n",
    "df_listings_data_cleaned.set_index('id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the missing values listings_data_cleaned\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "sns.heatmap(df_listings_data_cleaned.isnull(), cmap = 'Blues', cbar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_listings_data['neighbourhood_cleansed'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 85 **neighbourhood_cleansed** unique entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhoods = df_listings_data['neighbourhood_cleansed'].unique()\n",
    "neighbourhoods.sort()\n",
    "\n",
    "for neighbourhood in neighbourhoods: \n",
    "    print(neighbourhood.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching online for [Milan's neighbourhoods](http://www.museomilano.it/mediateca/media-pg-5/) and after some data cleaning we have this list of 130 neighbourhoods:\n",
    "- ticinese\n",
    "- magenta\n",
    "- porta vercellina\n",
    "- cordusio\n",
    "- carrobbio\n",
    "- cinquevie\n",
    "- sant’ambrogio\n",
    "- verziere\n",
    "- san babila\n",
    "- brolo-pantano\n",
    "- duomo\n",
    "- castello\n",
    "- sempione\n",
    "- brera\n",
    "- borgo degli ortolani - chinatown\n",
    "- porta nuova\n",
    "- centrale\n",
    "- centro direzionale\n",
    "- porta garibaldi\n",
    "- porta venezia\n",
    "- risorgimento\n",
    "- porta vittoria\n",
    "- porta romana\n",
    "- citta’ studi\n",
    "- acquabella\n",
    "- porta monforte\n",
    "- calvairate\n",
    "- lazio\n",
    "- tertulliano\n",
    "- porta vigentina\n",
    "- porta genova\n",
    "- porta lodovica\n",
    "- bullona\n",
    "- taliedo mecenate\n",
    "- morsenchio\n",
    "- gamboloita\n",
    "- castagnedo\n",
    "- vigentino\n",
    "- corvetto\n",
    "- nosedo\n",
    "- santa giulia\n",
    "- rogoredo\n",
    "- triulzo superiore\n",
    "- ponte lambro\n",
    "- forlanini\n",
    "- monluè\n",
    "- guastalla\n",
    "- ortica\n",
    "- cavriano\n",
    "- lambrate\n",
    "- loreto\n",
    "- abadesse\n",
    "- ponte seveso\n",
    "- isola\n",
    "- tortona\n",
    "- washington\n",
    "- solari\n",
    "- navigli\n",
    "- san pietro\n",
    "- la maddalena\n",
    "- pagano\n",
    "- fopponino\n",
    "- lotto\n",
    "- molinazzo\n",
    "- vaiano valle\n",
    "- selvanesco\n",
    "- moncucco\n",
    "- san cristoforo\n",
    "- lorenteggio giambellino\n",
    "- primaticcio \n",
    "- arzaga\n",
    "- forze armate\n",
    "- bisceglie\n",
    "- quarto cagnino\n",
    "- quinto romano\n",
    "- baggio\n",
    "- muggiano\n",
    "- trenno\n",
    "- figino\n",
    "- lampugnano\n",
    "- gallaratese\n",
    "- cascina merlata\n",
    "- certosa\n",
    "- qt8\n",
    "- san siro\n",
    "- portello\n",
    "- cagnola\n",
    "- musocco\n",
    "- roserio\n",
    "- vialba\n",
    "- ronchetto sul naviglio\n",
    "- barona\n",
    "- boffalora\n",
    "- chiesa rossa\n",
    "- conca fallata\n",
    "- cantalupa\n",
    "- gratosoglio\n",
    "- macconago\n",
    "- quintosole\n",
    "- morivione\n",
    "- chiaravalle\n",
    "- casoretto\n",
    "- greco\n",
    "- bicocca\n",
    "- prato centenario\n",
    "- gorla\n",
    "- precotto\n",
    "- villa san giovanni\n",
    "- adriano\n",
    "- crescenzago\n",
    "- rottole\n",
    "- turro\n",
    "- maggiolina\n",
    "- montalbino\n",
    "- niguarda\n",
    "- tre torri\n",
    "- dergano\n",
    "- affori \n",
    "- bovisasca\n",
    "- comasina\n",
    "- bruzzano\n",
    "- bovisa \n",
    "- villa pizzone\n",
    "- quarto oggiaro\n",
    "- farini \n",
    "- la fontana\n",
    "- ronchetto delle rane\n",
    "- conchetta\n",
    "- porta volta\n",
    "- ghisolfa\n",
    "\n",
    "![title](img/quartieri_milano.jpg)\n",
    "\n",
    "As we can see not all the neighbourhoods are rappresented in the dataset and moreover there is not an exact mapping between the dataset and the real neighbourhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_real_neighbourhood = []\n",
    "\n",
    "with open('quartieri.txt', 'r') as file:  \n",
    "    for line in file:\n",
    "        item = line.replace('\\n','') # remove linebreak\n",
    "        list_real_neighbourhood.append(item)\n",
    "        \n",
    "print(len(list_real_neighbourhood))\n",
    "#print(list_real_neighbourhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_present(item, lista):\n",
    "    for elemento in lista:\n",
    "        if elemento in item or item in elemento:\n",
    "            return elemento\n",
    "    return False\n",
    "\n",
    "lista = ['pippo', 'pluto', 'paperino']\n",
    "print(is_present('paperino', lista))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mapping_neighbourhood_real_neighbourhood = []\n",
    "list_no_matched_data_neighbourhood_by_real_neighbourhood = []\n",
    "list_no_matched_real_neighbourhood_by_data_neighbourhood = []\n",
    "\n",
    "for neighbourhood in neighbourhoods:\n",
    "    neighbourhood = neighbourhood.lower()\n",
    "    real_neighbourhood = is_present(neighbourhood, list_real_neighbourhood)\n",
    "    if real_neighbourhood == False:\n",
    "        list_no_matched_data_neighbourhood_by_real_neighbourhood.append(neighbourhood)\n",
    "    else:\n",
    "        list_mapping_neighbourhood_real_neighbourhood.append((neighbourhood, real_neighbourhood))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mapping_neighbourhood_real_neighbourhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By checking the association made by our funciton we can see some errors we must correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update wrong association ('ronchetto sul naviglio', 'navigli') and ('bovisa', 'bovisasca'),\n",
    "\n",
    "list_mapping_neighbourhood_real_neighbourhood_correct = []\n",
    "\n",
    "for i in range(len(list_mapping_neighbourhood_real_neighbourhood)):\n",
    "    if list_mapping_neighbourhood_real_neighbourhood[i][0] == 'ronchetto sul naviglio':\n",
    "        list_mapping_neighbourhood_real_neighbourhood_correct.append(('ronchetto sul naviglio', 'ronchetto sul naviglio'))\n",
    "    elif list_mapping_neighbourhood_real_neighbourhood[i][0] == 'bovisa':\n",
    "        list_mapping_neighbourhood_real_neighbourhood_correct.append(('bovisa', 'bovisa'))\n",
    "    else:\n",
    "        list_mapping_neighbourhood_real_neighbourhood_correct.append(list_mapping_neighbourhood_real_neighbourhood[i])\n",
    "        \n",
    "list_mapping_neighbourhood_real_neighbourhood = list_mapping_neighbourhood_real_neighbourhood_correct\n",
    "list_mapping_neighbourhood_real_neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neighbourhood in list_real_neighbourhood:\n",
    "    if neighbourhood not in [element[1] for element in list_mapping_neighbourhood_real_neighbourhood]:\n",
    "        list_no_matched_real_neighbourhood_by_data_neighbourhood.append(neighbourhood)\n",
    "        \n",
    "print(len(list_mapping_neighbourhood_real_neighbourhood))\n",
    "print(len(list_no_matched_data_neighbourhood_by_real_neighbourhood))\n",
    "print(len(list_no_matched_real_neighbourhood_by_data_neighbourhood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_no_matched_data_neighbourhood_by_real_neighbourhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_no_matched_real_neighbourhood_by_data_neighbourhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do by hand the mapping of this no matched neighbourhood:\n",
    "\n",
    "|     Data               |     Real                         |\n",
    "|------------------------|----------------------------------| \n",
    "| bande nere             | primaticcio                      |\n",
    "| buenos aires - venezia | porta venezia                    |\n",
    "| corsica                | acquabella                       |\n",
    "| de angeli - monte rosa | tre torri                        |\n",
    "| garibaldi repubblica   | porta garibaldi                  |\n",
    "| ortomercato            | calvairate                       |\n",
    "| padova                 | isola                            |\n",
    "| parco bosco in città   | quinto romano                    |\n",
    "| parco delle abbazie    | vaiano valle                     |\n",
    "| parco lambro - cimiano | lambrate                         |\n",
    "| parco nord             | bicocca                          |\n",
    "| qt 8                   | qt8                              |\n",
    "| ripamonti              | vigentino                        |\n",
    "| s. cristoforo          | san cristoforo                   |\n",
    "| s. siro                | san siro                         |\n",
    "| sacco                  | vialba                           |\n",
    "| sarpi                  | borgo degli ortolani - chinatown |\n",
    "| scalo romana           | vigentino                        |\n",
    "| selinunte              | san siro                         |\n",
    "| stadera                | chiesa rossa                     |\n",
    "| tibaldi                | conchetta                        |\n",
    "| umbria - molise        | calvairate                       |\n",
    "| viale monza            | gorla                            |\n",
    "| villapizzone           | villa pizzone                    |\n",
    "| xxii marzo             | porta vittoria                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_manual_mapping_neighbourhood_real_neighbourhood = [ ('bande nere', 'primaticcio')\n",
    "                                                        , ('buenos aires - venezia', 'porta venezia')\n",
    "                                                        , ('corsica', 'acquabella')\n",
    "                                                        , ('de angeli - monte rosa', 'tre torri')\n",
    "                                                        , ('garibaldi repubblica', 'porta garibaldi')\n",
    "                                                        , ('ortomercato', 'calvairate')\n",
    "                                                        , ('padova', 'isola')\n",
    "                                                        , ('parco bosco in citt\\x85', 'quinto romano')\n",
    "                                                        , ('parco delle abbazie', 'vaiano valle')\n",
    "                                                        , ('parco lambro - cimiano', 'lambrate')\n",
    "                                                        , ('parco nord', 'bicocca')\n",
    "                                                        , ('qt 8', 'qt8')\n",
    "                                                        , ('ripamonti', 'vigentino')\n",
    "                                                        , ('s. cristoforo', 'san cristoforo')\n",
    "                                                        , ('s. siro', 'san siro')\n",
    "                                                        , ('sacco', 'vialba')\n",
    "                                                        , ('sarpi', 'borgo degli ortolani - chinatown')\n",
    "                                                        , ('scalo romana', 'vigentin')\n",
    "                                                        , ('selinunte', 'san siro')\n",
    "                                                        , ('stadera', 'chiesa rossa')\n",
    "                                                        , ('tibaldi', 'conchetta')\n",
    "                                                        , ('umbria - molise', 'calvairate')\n",
    "                                                        , ('viale monza', 'gorla')\n",
    "                                                        , ('villapizzone', 'villa pizzone')\n",
    "                                                        , ('xxii marzo', 'porta vittoria')\n",
    "                                                       ]\n",
    "\n",
    "for tupla in list_manual_mapping_neighbourhood_real_neighbourhood:\n",
    "    list_mapping_neighbourhood_real_neighbourhood.append(tupla)\n",
    "    list_no_matched_data_neighbourhood_by_real_neighbourhood.remove(tupla[0])\n",
    "    if tupla[1] in list_no_matched_real_neighbourhood_by_data_neighbourhood:\n",
    "        list_no_matched_real_neighbourhood_by_data_neighbourhood.remove(tupla[1])\n",
    "    \n",
    "print(len(list_mapping_neighbourhood_real_neighbourhood))\n",
    "print(len(list_no_matched_data_neighbourhood_by_real_neighbourhood))\n",
    "print(len(list_no_matched_real_neighbourhood_by_data_neighbourhood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mapping_neighbourhood_real_neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not rappresented neighbourhood\n",
    "print(list_no_matched_real_neighbourhood_by_data_neighbourhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's map in the dataframe **neighbourhood_cleansed** to the real neighbourhoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_neighbourhood(data_neighbourhood, list_mapping):\n",
    "    for tupla in list_mapping:\n",
    "        if tupla[0] == data_neighbourhood:\n",
    "            return tupla[1]\n",
    "    return False\n",
    "\n",
    "print(get_real_neighbourhood('villapizzone', list_mapping_neighbourhood_real_neighbourhood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map neighbourhood to real neighbourhood\n",
    "df_listings_data_cleaned['real_neighbourhood'] = [get_real_neighbourhood(neighbourhood.lower(), list_mapping_neighbourhood_real_neighbourhood) for neighbourhood in df_listings_data_cleaned['neighbourhood_cleansed']]\n",
    "\n",
    "# Drop 'neighbourhood_cleansed' column\n",
    "df_listings_data_cleaned = df_listings_data_cleaned.drop(columns=['neighbourhood_cleansed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's detect the language of **neighborhood_overview**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blob = TextBlob('la casa è brutta')\n",
    "\n",
    "print(text_blob.detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return TextBlob(text).detect_language()\n",
    "    except:\n",
    "        return 'not detected'\n",
    "    \n",
    "print(detect_language(22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_neighborhood_overview_detected_language = []\n",
    "\n",
    "for neighborhood_overview in df_listings_data_cleaned['neighborhood_overview']:\n",
    "    list_neighborhood_overview_detected_language.append(detect_language(neighborhood_overview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_neighborhood_overview_detected_language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an histogram of the neighborhood_overview detected languages\n",
    "\n",
    "list_language_count = []\n",
    "\n",
    "for language in set(list_neighborhood_overview_detected_language): \n",
    "    list_language_count.append((language, list_neighborhood_overview_detected_language.count(language)))\n",
    "\n",
    "#print(list_language_count)\n",
    "\n",
    "languages = [language_count[0] for language_count in list_language_count]\n",
    "counts = [language_count[1] for language_count in list_language_count]\n",
    "\n",
    "plt.figure(figsize = (20, 10)) \n",
    "plt.bar(languages, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_review = df_listings_data_cleaned.shape[0]\n",
    "\n",
    "#print(total_number_review)\n",
    "\n",
    "for language, count in list_language_count:\n",
    "    print('{0:<15} {1:>8}'.format(language, count / total_number_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each neighborhood_overview with the detected language\n",
    "\n",
    "df_listings_data_cleaned['detected_language'] = [detect_language(neighborhood_overview) for neighborhood_overview in df_listings_data_cleaned['neighborhood_overview']]\n",
    "\n",
    "df_listings_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same analysis must be done on the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned = df_reviews_data[['listing_id'\n",
    "                                           #, 'date'\n",
    "                                           , 'comments']]\n",
    "\n",
    "df_reviews_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the missing values in reviews_data_cleaned\n",
    "\n",
    "# Not usefull because there is no missing value\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "sns.heatmap(df_reviews_data_cleaned.isnull(), cmap = 'Blues', cbar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_comment_detected_language = []\n",
    "\n",
    "for comment in df_reviews_data_cleaned['comments']:\n",
    "    list_comment_detected_language.append(detect_language(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_comment_detected_language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an histogram of the comment detected languages\n",
    "\n",
    "list_language_count = []\n",
    "\n",
    "for language in set(list_comment_detected_language): \n",
    "    list_language_count.append((language, list_comment_detected_language.count(language)))\n",
    "\n",
    "#print(list_language_count)\n",
    "\n",
    "languages = [language_count[0] for language_count in list_language_count]\n",
    "counts = [language_count[1] for language_count in list_language_count]\n",
    "\n",
    "plt.figure(figsize = (20, 10)) \n",
    "plt.bar(languages, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_review = df_reviews_data_cleaned.shape[0]\n",
    "\n",
    "#print(total_number_review)\n",
    "\n",
    "for language, count in list_language_count:\n",
    "    print('{0:<15} {1:>8}'.format(language, count / total_number_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each review with the detected language\n",
    "\n",
    "df_reviews_data_cleaned['detected_language'] = [detect_language(comment) for comment in df_reviews_data_cleaned['comments']]\n",
    "\n",
    "df_reviews_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe\n",
    "\n",
    "df_listings_data_cleaned.to_csv(data_folder + 'output_' + 'listings.csv')\n",
    "df_reviews_data_cleaned.to_csv(data_folder + 'output_' + 'reviews.csv')\n",
    "\n",
    "#df_listings_data_cleaned = pd.read_csv(data_folder + 'output_' + 'listings.csv')\n",
    "#df_reviews_data_cleaned = pd.read_csv(data_folder + 'output_' + 'reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a cursory overview of the reviews it seems that they are in different languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/\n",
    "\n",
    "def calculate_languages_ratios(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}\n",
    "    \n",
    "    @param word_tokens: Tokenized text whose language want to be detected\n",
    "    @type text: str\n",
    "    \n",
    "    @return: Dictionary with languages and unique stopwords seen in analyzed text\n",
    "    @rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    languages_ratios = {}\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_tokens = tokenizer.tokenize(text)\n",
    "    words = [word.lower() for word in word_tokens]\n",
    "\n",
    "    # Compute per language included in nltk number of unique stopwords appearing in analyzed text\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "\n",
    "    return languages_ratios\n",
    "\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return the highest scored.\n",
    "    \n",
    "    It uses a stopwords based approach, counting how many unique stopwords\n",
    "    are seen in analyzed text.\n",
    "    \n",
    "    @param text: Text whose language want to be detected\n",
    "    @type text: str\n",
    "    \n",
    "    @return: Most scored language guessed\n",
    "    @rtype: str\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        ratios = calculate_languages_ratios(text)\n",
    "        most_rated_language = max(ratios, key = ratios.get)\n",
    "    except:\n",
    "        most_rated_language = 'not detected'\n",
    "        \n",
    "    return most_rated_language\n",
    "\n",
    "\n",
    "#input_text = \"This is a sample sentence, showing off the language detection\"\n",
    "input_text = \"Questa è una frase in italiano\"\n",
    "\n",
    "print(detect_language(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_review_languages = []\n",
    "list_language_count = []\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "for comment in df_reviews_data_cleaned['comments']:\n",
    "    list_review_languages.append(detect_language(comment))\n",
    "\n",
    "for language in set(list_review_languages): \n",
    "    list_language_count.append((language, list_review_languages.count(language)))\n",
    "\n",
    "#print(list_review_languages)\n",
    "#print(list_language_count)\n",
    "\n",
    "languages = [language_count[0] for language_count in list_language_count]\n",
    "counts = [language_count[1] for language_count in list_language_count]\n",
    "\n",
    "plt.figure(figsize = (20, 10)) \n",
    "plt.bar(languages, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each review with the detected language\n",
    "\n",
    "df_reviews_data_cleaned['detected_language'] = [detect_language(comment) for comment in df_reviews_data_cleaned['comments']]\n",
    "\n",
    "df_reviews_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe\n",
    "\n",
    "df_listings_data_cleaned.to_csv(data_folder + 'output_' + 'listings.csv')\n",
    "df_reviews_data_cleaned.to_csv(data_folder + 'output_' + 'reviews.csv')\n",
    "\n",
    "#df_listings_data_cleaned = pd.read_csv(data_folder + 'output_' + 'listings.csv')\n",
    "#df_reviews_data_cleaned = pd.read_csv(data_folder + 'output_' + 'reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_review = df_reviews_data_cleaned.shape[0]\n",
    "\n",
    "#print(total_number_review)\n",
    "\n",
    "for language, count in list_language_count:\n",
    "    print('{0:<15} {1:>8}'.format(language, count / total_number_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of simplicity we can focus on Italian and English and meaby later extend our research to other languages.\n",
    "\n",
    "Synonyms neighborhood (Quartiere in Italian):\n",
    "- [`English`](https://www.thesaurus.com/browse/neighborhood):\n",
    " - area\n",
    " - block\n",
    " - district\n",
    " - ghetto\n",
    " - parish\n",
    " - part\n",
    " - precinct\n",
    " - region\n",
    " - section\n",
    " - slum\n",
    " - street\n",
    " - suburb\n",
    " - territory\n",
    " - zone\n",
    "- [`Italian`](https://dizionari.corriere.it/dizionario_sinonimi_contrari/Q/quartiere.shtml):\n",
    " - zona\n",
    " - vicinato\n",
    " - rione\n",
    " - sobborgo\n",
    " - borgata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisr let's try with English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned_eng = df_reviews_data_cleaned[df_reviews_data_cleaned['detected_language'] == 'english']\n",
    "\n",
    "df_reviews_data_cleaned_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_reviews_data_cleaned_eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's search in the review for the keywords related to neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_words_english = [ 'neighborhood'\n",
    "                          , 'area'\n",
    "                          , 'block'\n",
    "                          , 'district'\n",
    "                          , 'ghetto'\n",
    "                          , 'parish'\n",
    "                          #, 'part' # removed beacuse was beeing used to mark not neighborhood related part of comments\n",
    "                          , 'precinct'\n",
    "                          , 'region'\n",
    "                          , 'section'\n",
    "                          , 'slum'\n",
    "                          , 'street'\n",
    "                          , 'suburb'\n",
    "                          , 'territory'\n",
    "                          , 'zone'\n",
    "                          , 'location'\n",
    "                          ]\n",
    "\n",
    "#searched_words_italian = ['quartiere'\n",
    "#                          , 'zona'\n",
    "#                          , 'vicinato'\n",
    "#                          , 'rione'\n",
    "#                          , 'sobborgo'\n",
    "#                          , 'borgata'\n",
    "#                          ]\n",
    "\n",
    "#sarched_words  = searched_words_english + searched_words_italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_words(text, searched_words):\n",
    "    try:\n",
    "        for word in searched_words:\n",
    "            if word in text:\n",
    "                return True\n",
    "    except:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "print(detect_words('questo testo continene pippo', ['pippo', 'pluto']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reviews_data_cleaned_eng[['comments', 'detected_language']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize_text(text, language):\n",
    "    try:\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        word_tokens = tokenizer.tokenize(text)\n",
    "        stop_words = set(stopwords.words(language)) \n",
    "        filtered_sentence = [word for word in word_tokens if not word in stop_words] \n",
    "        return filtered_sentence\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "input_text = \"This is a sample sentence, showing off the stop words filtration!!!\"\n",
    "print(clean_tokenize_text(input_text, 'english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and clean comments\n",
    "comment, language = df_reviews_data_cleaned_eng[['comments','detected_language']].iloc[0]\n",
    "\n",
    "print(comment, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_tokenize_text(comment, language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detect_words(clean_tokenize_text(comment, language), searched_words_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each review if searched words are present\n",
    "\n",
    "list_key_serached_words_present = []\n",
    "\n",
    "for i in range (df_reviews_data_cleaned_eng.shape[0]):\n",
    "    comment, language = df_reviews_data_cleaned_eng[['comments','detected_language']].iloc[i]\n",
    "    list_key_serached_words_present.append((i, detect_words(clean_tokenize_text(comment, language), searched_words_english)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_key_serached_words_present))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serached_words_present = [key_serached_words_present[1] for key_serached_words_present in list_key_serached_words_present]\n",
    "\n",
    "print(sum(serached_words_present))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(serached_words_present) / df_reviews_data_cleaned_eng.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately only 18% of the english review contains some words related to the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned_eng['contains_serached_words'] = [key_serached_words_present[1] for key_serached_words_present in list_key_serached_words_present]\n",
    "\n",
    "df_reviews_data_cleaned_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned['contains_serached_words'] = [detect_words(clean_tokenize_text(row['comments'], row['detected_language']), searched_words_english) for index, row in df_reviews_data_cleaned]\n",
    "\n",
    "df_reviews_data_cleaned_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned_eng_contains = df_reviews_data_cleaned_eng[df_reviews_data_cleaned_eng['contains_serached_words'] == True]\n",
    "\n",
    "df_reviews_data_cleaned_eng_contains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the goal is to isolate the words related to neighborhood or similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = df_reviews_data_cleaned_eng_contains['comments'].iloc[0]\n",
    "\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at some comments I had realize I cuold use punctuation to isolate the phares related to neighborhood instad of remove it like I ws rhinking at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contextual_phrase(text, language, searched_words):\n",
    "    contextual_phrase = ''\n",
    "    sentences = text.split('.')\n",
    "    for sentence in sentences:\n",
    "        if detect_words(clean_tokenize_text(sentence, language), searched_words) == True:\n",
    "            contextual_phrase = contextual_phrase + ' ' + sentence\n",
    "    if contextual_phrase == '':\n",
    "        return text\n",
    "    else:\n",
    "        return contextual_phrase\n",
    "\n",
    "text = \"Staying at Francesca's and Alberto's place was a pleasure. Just as described, well located for my purposes, an enjoyable walk to the Tortona area. The room is very nice, cleaned daily and has private bathroom.Francesca is super friendly and very helpful; whilst still respecting privacy. Overall a great experience!\"\n",
    "\n",
    "print(get_contextual_phrase(text, 'english', searched_words_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_key_contextual_phrase = []\n",
    "\n",
    "for i in range (df_reviews_data_cleaned_eng_contains.shape[0]):\n",
    "    comment, language = df_reviews_data_cleaned_eng_contains[['comments','detected_language']].iloc[i]\n",
    "    list_key_contextual_phrase.append((i, get_contextual_phrase(comment, language, searched_words_english)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_key_contextual_phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned_eng_contains['contextual_phrase'] = [key_contextual_phrase[1] for key_contextual_phrase in list_key_contextual_phrase]\n",
    "\n",
    "df_reviews_data_cleaned_eng_contains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = df_reviews_data_cleaned_eng_contains['contextual_phrase'].iloc[0]\n",
    "\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from textblob import TextBlob\n",
    "\n",
    "text_blob = TextBlob('la casa è brutta')\n",
    "print(text_blob.detect_language())\n",
    "print(text_blob.tags)\n",
    "print(text_blob.words)\n",
    "print(text_blob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blob = TextBlob(comment)\n",
    "print(text_blob.tags)\n",
    "print(text_blob.words)\n",
    "print(text_blob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_key_sentiment = []\n",
    "\n",
    "for i in range (df_reviews_data_cleaned_eng_contains.shape[0]):\n",
    "    comment = df_reviews_data_cleaned_eng_contains['contextual_phrase'].iloc[i]\n",
    "    text_blob = TextBlob(comment)\n",
    "    list_key_sentiment.append((i, text_blob.sentiment.polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_key_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned_eng_contains['neighborhood_sentiment'] = [key_sentiment[1] for key_sentiment in list_key_sentiment]\n",
    "\n",
    "df_reviews_data_cleaned_eng_contains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reviews_data_cleaned_eng_contains.sort_values(by = 'neighborhood_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's peek at some records\n",
    "\n",
    "#comment = df_reviews_data_cleaned['comments'].iloc[325235]\n",
    "#print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_listing_sentiment = df_reviews_data_cleaned_eng_contains[['listing_id', 'neighborhood_sentiment']]\n",
    "\n",
    "df_reviews_listing_sentiment = df_reviews_listing_sentiment.groupby(['listing_id'], as_index = False)['neighborhood_sentiment'].mean()\n",
    "#df_reviews_listing_sentiment.set_index('listing_id', inplace = True)\n",
    "\n",
    "df_reviews_listing_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_listing_sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join reviews_listing_sentiment with original dataframe to link sentiment to the neighborhood\n",
    "\n",
    "df_listings_sentiment = df_listings_data_cleaned.join(df_reviews_listing_sentiment.set_index('listing_id'), on='id')\n",
    "\n",
    "df_listings_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment = df_listings_sentiment[['real_neighbourhood', 'neighborhood_sentiment']]\n",
    "\n",
    "df_neighbourhood_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment = df_neighbourhood_sentiment.groupby(['real_neighbourhood'], as_index = False)['neighborhood_sentiment'].mean()\n",
    "df_neighbourhood_sentiment.set_index('real_neighbourhood', inplace = True)\n",
    "df_neighbourhood_sentiment = df_neighbourhood_sentiment.sort_values(by = ['neighborhood_sentiment'], ascending=False)\n",
    "\n",
    "df_neighbourhood_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment.hist(column='neighborhood_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods = []\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df_neighbourhood_sentiment.iterrows():\n",
    "    neighborhoods.append(index)\n",
    "    sentiments.append(row['neighborhood_sentiment']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (40, 20)) \n",
    "plt.bar(neighborhoods, sentiments)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be usefull to compare the results of a sentiment analysis on the **neighborhood_overview** column in the listing dataframe but there are too many missing records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_listings_data_cleaned['neighborhood_overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_element = len(df_listings_data_cleaned['neighborhood_overview'])\n",
    "number_of_nan = df_listings_data_cleaned['neighborhood_overview'].isnull().sum()\n",
    "\n",
    "print(number_of_element)\n",
    "print(number_of_nan)\n",
    "print((number_of_nan / number_of_element) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's detect the **neighborhood_overview** languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "\n",
    "all_descriptions = ''\n",
    "\n",
    "for description in df_listings_data_cleaned['neighborhood_overview']:\n",
    "    if description:\n",
    "        all_descriptions = all_descriptions + ' ' + str(description)\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print('Elapsed time: {} seconds'.format(elapsed_time))\n",
    "print(len(all_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "all_descriptions = all_descriptions.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(len(all_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.w3resource.com/python-exercises/string/python-data-type-string-exercise-12.php\n",
    "\n",
    "def word_count(text):\n",
    "    counts = dict()\n",
    "    words = text.split(' ')\n",
    "\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in counts:\n",
    "            counts[word] = counts[word] + 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "print(word_count('the quick brown fox jumps over the lazy dog.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_word_count = word_count(all_descriptions)\n",
    "print(len(dictionary_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_descriptions = pd.DataFrame(list(dictionary_word_count.items()) , columns=['Word', 'Count'])\n",
    "#df_all_descriptions.set_index('Word', inplace = True)\n",
    "df_all_descriptions = df_all_descriptions.sort_values(by = ['Count'], ascending=False)\n",
    "df_all_descriptions.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_descriptions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_descriptions[df_all_descriptions['Count'] == 1].count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop words with frequency 1\n",
    "\n",
    "df_all_descriptions = df_all_descriptions[df_all_descriptions['Count'] > 1]\n",
    "\n",
    "# Looking at the words with the highest freequency we can see that we can use a threshold = 4000 to drop the most common words \n",
    "# like articles and conjunctions\n",
    "\n",
    "df_all_descriptions = df_all_descriptions[df_all_descriptions['Count'] < 3000]\n",
    "\n",
    "\n",
    "df_all_descriptions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_descriptions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
